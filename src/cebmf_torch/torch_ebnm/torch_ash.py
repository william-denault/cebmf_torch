# torch_convolved_loglik.py
import math
from typing import Optional

import torch

from cebmf_torch.utils.distribution_operation import (
    get_data_loglik_exp_torch,
    get_data_loglik_normal_torch,
)
from cebmf_torch.utils.mixture import (
    autoselect_scales_mix_exp,
    autoselect_scales_mix_norm,
    optimize_pi_logL,
)
from cebmf_torch.utils.posterior import posterior_mean_exp, posterior_mean_norm


class ash_object:
    def __init__(
        self,
        post_mean,
        post_mean2,
        post_sd,
        scale,
        pi0,
        prior,
        log_lik: float = 0.0,
        mode: float = 0.0,
    ):
        self.post_mean = post_mean
        self.post_mean2 = post_mean2
        self.post_sd = post_sd
        self.scale = scale
        self.pi0 = pi0
        self.prior = prior
        self.log_lik = log_lik
        self.mode = mode


# ---- ASH (Torch) ----
@torch.no_grad()
def ash(
    x,
    s,
    prior: str = "norm",
    mult: float = math.sqrt(2.0),
    penalty: float = 10.0,
    verbose: bool = True,
    threshold_loglikelihood: float = -300.0,
    mode: float = 0.0,
    *,
    batch_size: Optional[int] = 128,
    shuffle: bool = False,
    seed: Optional[int] = None,
):
    """
    Adaptive shrinkage with mixture priors ("norm" or "exp") in pure PyTorch.
    Uses EM for Ï€ (mini-batch capable via batch_size).
    Returns ash_object with Torch tensors.
    """

    # choose optimizer mode (EM by default here)

    s.clamp_(min=1e-12)
    if prior == "norm":
        scale = autoselect_scales_mix_norm(x, s, mult=mult)  # (K,)
        loc = torch.full((scale.shape[0],), float(mode), dtype=x.dtype, device=x.device)

        L = get_data_loglik_normal_torch(x, s, location=loc, scale=scale)  # (J,K)
        pi0 = optimize_pi_logL(
            L,
            penalty=penalty,
            verbose=verbose,
            batch_size=batch_size,
            shuffle=shuffle,
            seed=seed,
        )
        log_pi0 = torch.log(torch.clamp(pi0, min=1e-32))
        pm_obj = posterior_mean_norm(
            x, s, log_pi=log_pi0, data_loglik=L, location=loc, scale=scale
        )
        pm = pm_obj.post_mean
        pm2 = pm_obj.post_mean2
        psd = pm_obj.post_sd

    elif prior == "exp":
        scale = autoselect_scales_mix_exp(
            x, s, mult=mult
        )  # (K,) with scale[0]=0 (spike)
        L = get_data_loglik_exp_torch(x, s, scale=scale)  # (J,K)
        pi0 = optimize_pi_logL(
            L,
            penalty=penalty,
            verbose=verbose,
            batch_size=batch_size,
            shuffle=shuffle,
            seed=seed,
        )
        log_pi0 = torch.log(torch.clamp(pi0, min=1e-32))
        pm_obj = posterior_mean_exp(x, s, log_pi=log_pi0, scale=scale)
        pm = pm_obj.post_mean
        pm2 = pm_obj.post_mean2
        psd = pm_obj.post_sd

    else:
        raise ValueError("prior must be either 'norm' or 'exp'.")

    # total data log-likelihood: sum_j log sum_k pi_k * exp(L_{jk})
    Lc = torch.maximum(
        L, torch.tensor(threshold_loglikelihood, dtype=L.dtype, device=L.device)
    )
    log_lik_rows = torch.logsumexp(
        Lc + torch.log(torch.clamp(pi0, min=1e-300)).unsqueeze(0), dim=1
    )
    log_lik = float(log_lik_rows.sum().item())

    return ash_object(
        post_mean=pm,
        post_mean2=pm2,
        post_sd=psd,
        scale=scale,
        pi0=pi0[0],
        prior=prior,
        log_lik=log_lik,
        mode=float(mode),
    )
